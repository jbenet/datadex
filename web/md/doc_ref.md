
<!-- title: Reference -->
<!-- description: Command Reference for Data & Datadex -->


# data command reference

This document lists every data command (including subcommands), along with
its help page. It can be viewed by running 'data commands help', and
at http://datadex.io/doc/ref

Generated on 2014-01-17 19:35:01.385363354 +0000 UTC.

# data

```
data - dataset package manager

```

## data version

```
data version - Show data version information.

    Returns the current version of data and exits.

```

## data config

```
data config - Manage data configuration.

    Usage:

      data config <key> [<value>]

    Get or set configuration option values.
    If <value> argument is not provided, print <key> value, and exit.
    If <value> argument is provided, set <key> to <value>, and exit.

      # sets foo.bar = buzz
      > data config foo.bar baz

      # gets foo.bar
      > data config foo.bar
      baz

    Config options are stored in the user's configuration file (~/.dataconfig).
    This file is formatted in YAML, and uses the goyaml parser. (In the future,
    it may be formatted like .gitconfig (INI style), using the gcfg parser.)


```

## data info

```
data info - Show dataset information.

    Returns the Datafile corresponding to <dataset> (or in current
    directory) and exits.

```

## data list

```
data list - List insalled datasets.

		Returns all the datasets installed in the dataset working directory,
		end exits.

```

## data get

```
data get - Download and install dataset.

    Downloads the dataset specified, and installs its files into the
    current dataset working directory.

    The dataset argument can be any of:

    HANDLE: Handle of the form <author>/<name>[.<fmt>][@<ref>].
            Looks up handle on the specified (default) datadex.

    URL:    Direct url to any dataset on any datadex. (TODO)

    PATH:   Filesystem path to any locally installed dataset. (TODO)


    Loosely, data-get's process is:

    - Locate dataset Datafile and Manifest. (via provided argument).
    - Download Datafile and Manifest, to local Repository.
    - Download Blobs, listed in Manifest to local Repository.
    - Reconstruct Files, listed in Manifest.
    - Install Files, into working directory.


```

## data manifest

```
data manifest - Generate and manipulate dataset manifest.

    Generates and manipulates this dataset's manifest. The manifest
    is a mapping of { <path>: <checksum>}, and describes all files
    that compose a dataset. This mapping is generated by adding and
    hashing (checksum) files.

    Running data-manifest without arguments will generate (or patch)
    the manifest. Note that already hashed files will not be re-hashed
    unless forced to. Some files may be massive, and hashing every run
    would be prohibitively expensive.

    Commands:

      add <file>      Adds <file> to manifest (does not hash).
      rm <file>       Removes <file> from manifest.
      hash <file>     Hashes <file> and adds checksum to manifest.
      check <file>    Verifies <file> checksum matches manifest.

    (use the --all flag to do it to all available files)

    Loosely, data-manifest's process is:

    - List all files in the working directory.
    - Add files to the manifest (effectively tracking them).
    - Hash tracked files, adding checksums to the manifest.

```

### data manifest add

```
data manifest add - Adds <file> to manifest (does not hash).

    Adding files to the manifest ensures they are tracked. This command
    adds the given <file> to the manifest, saves it, and exits. It does
    not automatically hash the file (run 'data manifest hash').

    See 'data manifest'.

Arguments:

    <file>   path of the file to add.


```

### data manifest rm

```
data manifest rm - Removes <file> from manifest.

    Removing files from the manifest stops tracking them. This command
    removes the given <file> (and hash) from the manifest, and exits.

    See 'data manifest'.

Arguments:

    <file>   path of the file to remove.


```

### data manifest hash

```
data manifest hash - Hashes <file> and adds checksum to manifest.

		Hashing files in the manifest calculates the file checksums. This command
    hashes the given <file>, adds it to the manifest, and exits.

    See 'data manifest'.

Arguments:

    <file>   path of the file to hash.


```

### data manifest check

```
data manifest check - Verifies <file> checksum matches manifest.

		The manifest lists the files and their checksums. This command
    hashes the given <file>, and prints whether its checksum matches the
    stored checksum.

    See 'data manifest'.

Arguments:

    <file>   path of the file to check.


```

## data pack

```
data pack - Dataset packaging, upload, and download.

  Commands:

      pack make       Create or update package description.
      pack manifest   Show current package manifest.
      pack upload     Upload package to remote storage.
      pack download   Download package from remote storage.
      pack publish    Publish package to dataset index.
      pack checksum   Verify all file checksums match.


  What is a data package?

    A data package represents a single dataset, a unit of information.
    data makes it easy to find, download, create, publish, and maintain
    these datasets/packages.

    Dataset packages are simply file directories with two extra files:
    - Datafile, containing dataset description and metadata
    - Manifest, containing dataset file paths and checksums
    (See 'data help datafile' and 'data help manifest'.)

  data pack make

    'Packing' is the process of generating the package's Datafile and
    Manifest. The Manifest is built automatically, but the Datafile
    requires user input, to specify name, author, description, etc.

  data pack manifest

    Shows the current package manifest. This may be out of date with the
    current directory contents.

  data pack upload

    Packages, once 'packed' (Datafile + Manifest created), can be uploaded
    to a remote storage service (by default, the datadex). This means
    uploading all the package's files (blobs) not already present in the
    storage service. This is determined using a checksum.

  data pack download

    Similarly, packages can be downloaded or reconstructed in any directory
    from the Datafile and Manifest. Running 'data pack download' ensures
    all files listed in the Manifest are downloaded to the directory.

  data pack publish

    Packages can be published to the dataset index. Running 'data pack
    publish' posts the current manifest reference (hash) to the index.
    The package should already be uploaded (to the storage service).
    Publishing requires index credentials (see 'data user').

  data pack checksum

    Packages can be verified entirely by calling the 'data pack checksum'
    command. It re-hashes every file and ensures the checksums match.

```

### data pack make

```
data pack upload - Upload package contents to remote storage.

    Makes the package's description files:
    - Datafile, containing dataset description and metadata (prompts)
    - Manifest, containing dataset file paths and checksums (generated)

    See 'data pack'.

```

### data pack manifest

```
data pack manifest - Show current package manifest.

    Shows the package's manifest file and exits.
    If no manifest file exists, exit with an error.

    See 'data pack'.

```

### data pack upload

```
data pack upload - Upload package contents to remote storage.

    Uploads package's files (blobs) to a remote storage service (datadex).
    Blobs are named by their hash (checksum), so data can deduplicate.
    Meaning, data can easily tell whether the service already has each
    file, avoiding redundant uploads, saving bandwidth, and leveraging
    the data uploaded along with other datasets.

    See 'data pack'.

```

### data pack download

```
data pack download - Download package contents from remote storage.

    Downloads package's files (blobs) from remote storage service (datadex).
    Blobs are named by their hash (checksum), so data can deduplicate and
    ensure integrity. Meaning, data can avoid redundant downloads, saving
    bandwidth and speed, as well as verify the correctness of files with
    their checksum, preventing corruption.

    See 'data pack'.

```

### data pack publish

```
data pack publish - Publish package reference to dataset index.

    Publishes pckage's manifest reference (hash) to the dataset index.
    Package manifest (and all blobs) should be already uploaded. If any
    blob has not been uploaded, publish will exit with an error.

    Note: publishing requires data index credentials; see 'data user'.

    See 'data pack'.

```

### data pack check

```
data pack check - Verify all file checksums match.

    Verifies all package's file (blob) checksums match hashes stored in
    the Manifest. This is the way to check package-wide integrity. If any
    checksums FAIL, it is suggested that the files be re-downloaded (using
    'data pack download' or 'data blob get <hash>').

    See 'data pack'.

```

## data blob

```
data blob - Manage blobs in the blobstore.

  Commands:

    put <hash> <path>     Upload blob named by <hash> to blobstore.
    get <hash> <path>     Download blob named by <hash> from blobstore.
    check <hash> <path>   Verify blob matches <hash>.
    url <hash>            Output Url for blob named by <hash>.
    show <hash>           Output blob contents for hash.
    hash <path>           Output hash for blob contents.

  Arguments:

    The <hash> argument is the blob's checksum, and id.
    The <path> argument is the blob's target file.
    If <path> is omitted, stdin/stdout are used.


  What is a blob?

    Datasets are made up of files, which are made up of blobs.
    (For now, 1 file is 1 blob. Chunking to be implemented)
    Blobs are basically blocks of data, which are checksummed
    (for integrity, de-duplication, and addressing) using a crypto-
    graphic hash function (sha1, for now). If git comes to mind,
    that's exactly right.

  Local Blobstores

    data stores blobs in blobstores. Every local dataset has a
    blobstore (local caching with links TBI). Like in git, the blobs
    are stored safely in the blobstore (different directory) and can
    be used to reconstruct any corrupted/deleted/modified dataset files.

  Remote Blobstores

    data uses remote blobstores to distribute datasets across users.
    The datadex service includes a blobstore (currently an S3 bucket).
    By default, the global datadex blobstore is where things are
    uploaded to and retrieved from.

    Since blobs are uniquely identified by their hash, maintaining one
    global blobstore helps reduce data redundancy. However, users can
    run their own datadex service. (The index and blobstore are tied
    together to ensure consistency. Please do not publish datasets to
    an index if blobs aren't in that index)

    data can use any remote blobstore you wish. (For now, you have to
    recompile, but in the future, you will be able to) Just change the
    datadex configuration variable. Or pass in "-s <url>" per command.

    (data-blob is part of the plumbing, lower level tools.
    Use it directly if you know what you're doing.)

```

### data blob put

```
data blob put - Upload blobs to a remote blobstore.

    Upload the blob contents named by <hash> to a remote blobstore.
    Blob contents are stored locally, to be used to reconstruct files.
    In the future, the blobstore will be able to be changed. For now,
    the default blobstore/datadex is used.

    See data blob.

Arguments:

    <hash>   name (cryptographic hash, checksum) of the blob.
    <path>   path of the blob contents to upload.


```

### data blob get

```
data blob get - Download blobs from a remote blobstore.

    Download the blob contents named by <hash> from a remote blobstore.
    Blob contents are stored locally, to be used to reconstruct files.
    In the future, the blobstore will be able to be changed. For now,
    the default blobstore/datadex is used.

    See data blob.

Arguments:

    <hash>   name (cryptographic hash, checksum) of the blob.
    <path>   path to put the blob contents in.


```

### data blob url

```
data blob url - Output Url for blob named by <hash>.

    Output the remote storage url for the blob contents named by <hash>.
    In the future, the blobstore will be able to be changed. For now,
    the default blobstore/datadex is used.

    See data blob.

Arguments:

    <hash>   name (cryptographic hash, checksum) of the blob.


```

### data blob show

```
data blob show - Output blob contents for hash.

    Output the blob contents stored in the blobstore for hash.
    If the blob is available locally, that copy is used (after
    hashing to verify correctness). Otherwise, it is downloaded
    from the blobstore.

    See data blob.

Arguments:

    <hash>   name (cryptographic hash, checksum) of the blob.


```

### data blob hash

```
data blob hash - Output hash for blob contents.

    Output the hash of the blob contents stored in <path>

    See data blob.

Arguments:

    <path>   path of the blob contents


```

### data blob check

```
data blob check - Verify blob matches <hash>.

    Verify the hash of the blob contents stored in <path> matches
    <hash>.

    See data blob.

Arguments:

    <hash>   name (cryptographic hash, checksum) of the blob.
    <path>   path of the blob contents


```

## data publish

```
data publish - Guided dataset publishing.

    This command guides the user through the necessary steps to
    create a data package (Datafile and Manifest), uploads it,
    and publishes it to the dataset index.

    See 'data pack'.

```

## data user

```
data user - Manage users and credentials.

    Usage:

      data user <command> <username>

    Commands:

      add [<username>]   Register new user with index.
      auth [<username>]  Authenticate user account.
      pass [<username>]  Change user password.
      info [<username>]  Show (or edit) public user information.
      url [<username>]   Output user profile url.

    If no argument is provided, data will ask for the username.

    User accounts are needed in order to publish dataset packages to the
    dataset index. Packages are listed under their owner's username:
    '<owner>/<dataset>'.


```

### data user add

```
data user add - Register new user with index.

    Guided process to register a new user account with dataset index.

    See data user.

```

### data user auth

```
data user auth - Authenticate user account.

    Authenticate (login) user account to index. An auth token is retrieved
    and stored in the local config file.

    See data user.

```

### data user pass

```
data user pass - Change user password.

    Guided process to change user account password with dataset index.

    See data user.

```

### data user info

```
data user info - Show (or edit) public user information.

    Output or edit the profile information of a user. Note that profiles
    are publicly viewable. User profiles include:

      Full Name
      Email Address
      Github Username
      Twitter Username
      Website Url
      Packages List

    See data user.

```

### data user url

```
data user url - Output user profile url.

    Output the dataset index url for the profile of user named by <username>.

    See data user.

```

## data commands

```
data commands - List all available commands.

    Lists all available commands (and sub-commands) and exits.

```

### data commands help

```
data commands help - List all available commands's help pages.

    Shows the pages of all available commands (and sub-commands) and exits.
    Outputs a markdown document, also viewable at http://datadex.io/doc/ref

```
